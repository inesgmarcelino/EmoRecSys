{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import NMF\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "ratings_df = pd.read_csv(\"../data/csvs/ratings.csv\", sep=',', encoding='latin-1',low_memory=False)\n",
    "#photos_df = pd.read_csv('C:/Users/lmir/Desktop/Tese/Results_sim/emorecsys_data/CSVs_8_7_2024/csvs_survey/photos.csv', sep=',', encoding='latin-1',low_memory=False)\n",
    "\n",
    "demo_df = pd.read_csv('../data/csvs/demographic.csv')\n",
    "\n",
    "pixel_sim_resnet152_df = pd.read_csv('../data/csvs/pixel_sim_resnet152.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target ID</th>\n",
       "      <th>Comparison ID</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2677.0</td>\n",
       "      <td>2677.0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2135.0</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1816.0</td>\n",
       "      <td>1816.0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1341.0</td>\n",
       "      <td>1341.0</td>\n",
       "      <td>1.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153176</th>\n",
       "      <td>2901.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>0.159288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153177</th>\n",
       "      <td>1858.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>0.156662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153178</th>\n",
       "      <td>2922.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>0.149955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153179</th>\n",
       "      <td>1858.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>0.149714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153180</th>\n",
       "      <td>2895.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>0.147433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153181 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target ID  Comparison ID  Similarity Score\n",
       "0           193.0          193.0          1.000001\n",
       "1          2677.0         2677.0          1.000001\n",
       "2          2135.0         2135.0          1.000001\n",
       "3          1816.0         1816.0          1.000001\n",
       "4          1341.0         1341.0          1.000001\n",
       "...           ...            ...               ...\n",
       "153176     2901.0         1223.0          0.159288\n",
       "153177     1858.0          652.0          0.156662\n",
       "153178     2922.0         1223.0          0.149955\n",
       "153179     1858.0         1223.0          0.149714\n",
       "153180     2895.0         1223.0          0.147433\n",
       "\n",
       "[153181 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_sim_resnet152_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_items(trainset, testset):\n",
    "  trainset_items = set(list(item for _, item, _ in trainset))\n",
    "  testset_items = set(list(item for _, item, _ in testset))\n",
    "\n",
    "  items_unknown = list(testset_items - trainset_items)\n",
    "\n",
    "  return len(items_unknown) == 0\n",
    "\n",
    "def split_emorecsys(dataset, train=0.8, test=0.2):\n",
    "  trainset, testset = list(), list()\n",
    "\n",
    "  items_ratings = dataset.groupby('id_photo').size().to_dict() # get number of rating for each id_photo\n",
    "  items_one_rating = [id_photo for id_photo, size in items_ratings.items() if size == 1] # get the ones with only 1 rating\n",
    "  # print(items_one_rating)\n",
    "\n",
    "\n",
    "  for user in list(set(dataset['id_survey'])):\n",
    "    user_ratings = dataset[dataset['id_survey'] == user]\n",
    "    all_ratings = list((user, id_photo, like_bool) for id_photo, like_bool in zip(user_ratings['id_photo'], user_ratings['like_bool']))\n",
    "    size_ratings = len(all_ratings)\n",
    "\n",
    "    user_trainset = list(rating for rating in all_ratings if rating[1] in items_one_rating) # start trainset with items with only one rating\n",
    "\n",
    "    relevant = list(x for x in all_ratings if x[2] == 1)\n",
    "    if relevant: # making sure that we have at least one relevant rating in the testset\n",
    "      new_rating = random.sample(relevant, 1)\n",
    "      user_testset = new_rating if new_rating[0] not in user_trainset else list()\n",
    "    else:\n",
    "      raise ValueError('The user ' + str(user) + ' did not liked any photo')\n",
    "\n",
    "    while len(user_testset) < size_ratings*test: # add random ratings until fullfil the size of testset\n",
    "      new_rating = random.choice(all_ratings)\n",
    "      if new_rating not in user_testset and new_rating not in user_trainset:\n",
    "        user_testset.append(new_rating)\n",
    "\n",
    "    # adding remaining ratings to trainset\n",
    "    user_trainset.extend(new_rating for new_rating in all_ratings if new_rating not in user_testset and new_rating not in user_trainset)\n",
    "\n",
    "    assert len(user_trainset) == size_ratings*train\n",
    "    assert len(user_testset) == size_ratings*test\n",
    "\n",
    "    # print(user_trainset)\n",
    "    # print(user_testset)\n",
    "\n",
    "    trainset.extend(user_trainset)\n",
    "    testset.extend(user_testset)\n",
    "\n",
    "  if check_items(trainset, testset):\n",
    "    train_df = pd.DataFrame(trainset, columns=['id_survey', 'id_photo', 'like_bool']) # like_bool\n",
    "  else:\n",
    "    return split_emorecsys(dataset, train, test)\n",
    "\n",
    "  reader = Reader(rating_scale=(0,1)) # like_bool\n",
    "  surprise_train = Dataset.load_from_df(train_df, reader).build_full_trainset()\n",
    "\n",
    "  return surprise_train, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_train, testset = split_emorecsys(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<surprise.trainset.Trainset object at 0x000001BCD58FE410>\n",
      "[(1, 1785, 1), (1, 1507, 1), (1, 948, 0), (2, 2370, 1), (2, 342, 0), (2, 1898, 0), (3, 2184, 1), (3, 1747, 1), (3, 655, 1), (4, 1816, 1), (4, 1082, 1), (4, 2933, 1), (5, 749, 1), (5, 2067, 1), (5, 2717, 1), (6, 681, 1), (6, 546, 1), (6, 2581, 1), (7, 429, 1), (7, 2789, 1), (7, 2195, 1), (8, 426, 1), (8, 1925, 0), (8, 38, 1), (9, 2193, 0), (9, 2668, 1), (9, 1712, 1), (10, 55, 1), (10, 321, 0), (10, 732, 1), (11, 1821, 1), (11, 1681, 1), (11, 2951, 1), (12, 1721, 1), (12, 2876, 0), (12, 1838, 1), (13, 2787, 1), (13, 142, 0), (13, 1089, 1), (14, 612, 1), (14, 1342, 1), (14, 2097, 1), (15, 1312, 1), (15, 2848, 1), (15, 407, 1), (16, 2410, 1), (16, 993, 0), (16, 2135, 1), (17, 791, 1), (17, 2811, 0), (17, 2804, 1), (18, 2536, 1), (18, 1550, 1), (18, 474, 1), (19, 1801, 1), (19, 1535, 1), (19, 1891, 1), (20, 2796, 1), (20, 2444, 1), (20, 1013, 0), (21, 137, 1), (21, 1012, 1), (21, 2792, 1), (22, 1428, 1), (22, 2369, 1), (22, 1910, 1), (23, 861, 1), (23, 2341, 1), (23, 534, 0), (24, 1487, 1), (24, 186, 1), (24, 2159, 1), (25, 1201, 1), (25, 278, 0), (25, 1454, 1), (26, 1588, 1), (26, 2742, 1), (26, 699, 1), (27, 1966, 1), (27, 2352, 1), (27, 2366, 1), (28, 414, 1), (28, 1362, 1), (28, 46, 0), (29, 2755, 1), (29, 1939, 1), (29, 2219, 1), (30, 2409, 1), (30, 550, 0), (30, 248, 0), (31, 2258, 1), (31, 1427, 1), (31, 864, 1), (32, 1982, 1), (32, 2368, 1), (32, 2127, 1), (33, 767, 1), (33, 2660, 0), (33, 390, 1), (34, 2630, 1), (34, 1034, 1), (34, 1812, 1), (35, 2344, 1), (35, 1500, 0), (35, 2202, 1), (36, 1642, 1), (36, 1871, 0), (36, 2937, 1), (37, 1254, 1), (37, 1881, 0), (37, 1818, 1), (38, 1308, 1), (38, 2369, 0), (38, 1621, 1), (39, 1621, 1), (39, 736, 1), (39, 2442, 1), (40, 2369, 1), (40, 3057, 0), (40, 1308, 1), (41, 1327, 1), (41, 2894, 0), (41, 1080, 0), (42, 2894, 1), (42, 471, 1), (42, 667, 1), (43, 1848, 1), (43, 667, 1), (43, 2193, 0), (44, 1407, 1), (44, 2, 1), (44, 1848, 1), (45, 1417, 1), (45, 248, 0), (45, 471, 1), (46, 390, 1), (46, 2, 0), (46, 1407, 0), (47, 1818, 1), (47, 471, 1), (47, 2, 1), (48, 667, 1), (48, 471, 1), (48, 248, 0), (49, 1818, 1), (49, 1417, 1), (49, 113, 1), (50, 2, 1), (50, 1738, 1), (50, 1165, 0), (51, 667, 1), (51, 248, 0), (51, 1417, 1), (52, 1279, 1), (52, 216, 1), (52, 1082, 1), (53, 1495, 1), (53, 216, 1), (53, 1034, 0), (54, 2206, 1), (54, 1805, 0), (54, 216, 1), (55, 1805, 1), (55, 1082, 1), (55, 2353, 1), (56, 271, 1), (56, 1546, 1), (56, 1365, 1), (57, 645, 1), (57, 2618, 0), (57, 1365, 1), (58, 890, 1), (58, 1546, 1), (58, 2963, 1), (59, 2963, 1), (59, 1546, 0), (59, 1761, 0), (60, 2583, 1), (60, 2409, 1), (60, 1388, 1), (61, 1388, 1), (61, 2408, 0), (61, 99, 1), (62, 2914, 1), (62, 346, 0), (62, 2121, 1), (63, 2409, 1), (63, 2050, 0), (63, 1089, 1), (64, 1538, 1), (64, 750, 0), (64, 2405, 1), (65, 205, 1), (65, 2410, 1), (65, 925, 0), (66, 2111, 1), (66, 2581, 1), (66, 227, 0), (67, 2111, 1), (67, 2998, 1), (67, 250, 0), (68, 2581, 1), (68, 1055, 1), (68, 2998, 0), (69, 205, 1), (69, 1833, 0), (69, 2481, 1), (70, 1538, 1), (70, 1833, 1), (70, 925, 0), (71, 2405, 1), (71, 205, 1), (71, 1910, 0), (72, 1461, 1), (72, 1737, 1), (72, 148, 0), (73, 1737, 1), (73, 6, 0), (73, 1730, 1), (74, 6, 1), (74, 1461, 1), (74, 3042, 0), (75, 461, 1), (75, 1737, 1), (75, 2368, 1), (76, 1362, 1), (76, 732, 1), (76, 2161, 1), (77, 438, 1), (77, 2161, 1), (77, 2259, 0), (78, 2387, 1), (78, 1427, 1), (78, 2259, 1), (79, 2098, 1), (79, 438, 1), (79, 732, 1), (80, 2323, 1), (80, 1513, 1), (80, 62, 1), (81, 2892, 1), (81, 80, 1), (81, 655, 1), (82, 1254, 1), (82, 3032, 0), (82, 2892, 1), (83, 567, 1), (83, 2933, 1), (83, 128, 0), (84, 2660, 1), (84, 567, 1), (84, 1335, 1), (85, 655, 1), (85, 1513, 1), (85, 174, 1), (86, 1521, 1), (86, 2839, 0), (86, 2323, 0), (87, 2789, 1), (87, 1513, 1), (87, 954, 1), (88, 2734, 1), (88, 347, 0), (88, 1840, 0), (89, 2559, 1), (89, 1840, 0), (89, 1091, 1), (90, 1840, 1), (90, 1091, 1), (90, 278, 1), (91, 278, 1), (91, 738, 1), (91, 2734, 1), (92, 474, 1), (92, 524, 0), (92, 1311, 0), (93, 60, 1), (93, 1311, 0), (93, 993, 1), (94, 1311, 1), (94, 1395, 1), (94, 524, 0), (95, 1223, 1), (95, 749, 1), (95, 524, 1), (96, 749, 1), (96, 524, 1), (96, 2347, 1), (97, 2347, 1), (97, 2416, 1), (97, 426, 1), (98, 1273, 1), (98, 426, 1), (98, 1013, 0), (99, 498, 1), (99, 598, 1), (99, 2745, 0), (100, 1273, 1), (100, 2448, 1), (100, 1612, 0), (101, 2095, 1), (101, 2159, 1), (101, 766, 0), (102, 2159, 1), (102, 215, 1), (102, 2087, 1), (103, 215, 1), (103, 2448, 1), (103, 94, 1), (104, 769, 1), (104, 675, 1), (104, 379, 0), (105, 2482, 1), (105, 675, 1), (105, 379, 0), (106, 1642, 1), (106, 2287, 1), (106, 766, 0), (107, 2951, 1), (107, 2406, 1), (107, 1812, 1), (108, 692, 1), (108, 1496, 1), (108, 231, 0), (109, 556, 1), (109, 2951, 1), (109, 231, 0), (110, 2372, 1), (110, 2895, 0), (110, 2955, 0), (111, 644, 1), (111, 953, 1), (111, 1869, 0), (112, 2895, 1), (112, 2476, 0), (112, 1891, 1), (113, 481, 1), (113, 2370, 1), (113, 2895, 1), (114, 2258, 1), (114, 208, 1), (114, 2127, 1), (115, 1374, 1), (115, 2848, 1), (115, 1691, 1), (116, 2848, 1), (116, 14, 1), (116, 1691, 1), (117, 2738, 1), (117, 14, 1), (117, 1374, 1), (118, 706, 1), (118, 1078, 1), (118, 2144, 1), (119, 1112, 1), (119, 364, 0), (119, 1340, 1), (120, 857, 1), (120, 2144, 1), (120, 756, 1), (121, 2989, 1), (121, 2552, 1), (121, 857, 0), (122, 681, 1), (122, 864, 1), (122, 2552, 1), (123, 1487, 1), (123, 1340, 1), (123, 2621, 1), (124, 429, 1), (124, 980, 0), (124, 864, 0), (125, 550, 1), (125, 1487, 1), (125, 2736, 1), (126, 1487, 1), (126, 546, 1), (126, 681, 1), (127, 1352, 1), (127, 2876, 1), (127, 1247, 0), (128, 2877, 1), (128, 177, 1), (128, 310, 1), (129, 310, 1), (129, 177, 1), (129, 1359, 1), (130, 2211, 1), (130, 2313, 1), (130, 557, 1), (131, 98, 1), (131, 2288, 1), (131, 2474, 0), (132, 1712, 1), (132, 1748, 1), (132, 1543, 0), (133, 1748, 1), (133, 1966, 1), (133, 342, 0), (134, 2219, 1), (134, 1712, 1), (134, 1627, 0), (135, 767, 1), (135, 298, 1), (135, 1543, 0), (136, 1424, 1), (136, 835, 1), (136, 1352, 1), (137, 1982, 1), (137, 2288, 1), (137, 2474, 0), (138, 1982, 1), (138, 2002, 0), (138, 2474, 1), (139, 2780, 1), (139, 38, 1), (139, 2303, 1), (140, 845, 1), (140, 634, 0), (140, 1197, 0), (141, 2067, 1), (141, 476, 1), (141, 1816, 1), (142, 476, 1), (142, 835, 0), (142, 2067, 1), (143, 2202, 1), (143, 98, 1), (143, 835, 1), (144, 421, 1), (144, 1881, 0), (144, 142, 0), (145, 3047, 1), (145, 1881, 1), (145, 414, 1), (146, 421, 1), (146, 184, 1), (146, 2796, 0), (147, 414, 1), (147, 1881, 1), (147, 3047, 0), (148, 184, 1), (148, 2435, 0), (148, 118, 0), (149, 1717, 1), (149, 2751, 1), (149, 1151, 1), (150, 2669, 1), (150, 1721, 1), (150, 1925, 0), (151, 1644, 1), (151, 1925, 0), (151, 1717, 1), (152, 2234, 1), (152, 1721, 1), (152, 2604, 0), (153, 1644, 1), (153, 2604, 1), (153, 136, 1), (154, 791, 1), (154, 1554, 1), (154, 1939, 1), (155, 2412, 1), (155, 136, 1), (155, 1853, 0), (156, 39, 1), (156, 1280, 1), (156, 2006, 0), (157, 2412, 1), (157, 1280, 1), (157, 136, 1), (158, 2412, 1), (158, 1922, 1), (158, 1391, 1), (159, 652, 1), (159, 1588, 1), (159, 1518, 1), (160, 2097, 1), (160, 1898, 1), (160, 652, 1), (161, 2630, 1), (161, 464, 1), (161, 1898, 1), (162, 2053, 1), (162, 1726, 0), (162, 365, 0), (163, 32, 1), (163, 2546, 0), (163, 1898, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(surprise_train)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmir\\AppData\\Local\\Temp\\ipykernel_23588\\2714364257.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  user_profiles = user_profiles.applymap(lambda x: 1 if x is True else 0 if x is False else x)\n"
     ]
    }
   ],
   "source": [
    "## preprocessing for Demographic dataset\n",
    "user_profiles = demo_df.iloc[:, -27:].copy()\n",
    "\n",
    "## one-hot encoder\n",
    "for col in ['hobby_other', 'country_residence', 'city', 'education', 'gender', 'populational_aff', 'age']:\n",
    "  dummies = pd.get_dummies(demo_df[col], prefix=col)\n",
    "  user_profiles = pd.concat([dummies, user_profiles], axis=1)\n",
    "\n",
    "user_profiles.dropna(inplace=True)\n",
    "user_profiles = user_profiles.applymap(lambda x: 1 if x is True else 0 if x is False else x)\n",
    "\n",
    "## applying PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(user_profiles)\n",
    "tve = 0 # total variance explained\n",
    "for i, ve in enumerate(pca.explained_variance_ratio_):\n",
    "  tve += ve\n",
    "  # print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i+1, ve, tve))\n",
    "\n",
    "## keep 32 principal components, since we get a total explained variance of 90.13%.\n",
    "X_pca = pca.transform(user_profiles)[:, :32]\n",
    "\n",
    "## see dataframe\n",
    "user_profiles_train = pd.DataFrame(data=X_pca, columns=['PCA'+str(i) for i in range(1, X_pca.shape[1]+1)], index=demo_df['id_survey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we will be creating a ratings_matrix, in this case using the `like_bool` as the rating\n",
    "ratings_matrix_like = ratings_df.pivot_table(index='id_survey', columns='id_photo', values='like_bool').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id_photo</th>\n",
       "      <th>2</th>\n",
       "      <th>6</th>\n",
       "      <th>14</th>\n",
       "      <th>25</th>\n",
       "      <th>32</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>46</th>\n",
       "      <th>55</th>\n",
       "      <th>...</th>\n",
       "      <th>3032</th>\n",
       "      <th>3037</th>\n",
       "      <th>3041</th>\n",
       "      <th>3042</th>\n",
       "      <th>3047</th>\n",
       "      <th>3054</th>\n",
       "      <th>3057</th>\n",
       "      <th>3059</th>\n",
       "      <th>3062</th>\n",
       "      <th>3068</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows Ã— 553 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id_photo  2     6     14    25    32    38    39    40    46    55    ...  \\\n",
       "0          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "1          NaN   NaN   NaN   0.0   NaN   NaN   NaN   1.0   NaN   NaN  ...   \n",
       "2          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "..         ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "158        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "159        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "160        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "161        NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "162        NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "id_photo  3032  3037  3041  3042  3047  3054  3057  3059  3062  3068  \n",
       "0          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "..         ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "158        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "159        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "160        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "161        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "162        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[163 rows x 553 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_matrix_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_params = {'n_epochs': 20, 'n_factors': 100, 'reg_pu': 0.001, 'reg_qi': 0.01} # nmf\n",
    "\n",
    "demo_params = {'affinity': 'nearest_neighbors', 'assign_labels': 'kmeans', 'eigen_solver': 'lobpcg', 'gamma': 2.9, 'n_clusters': 20, 'n_neighbors': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_users(user, model, n=5, model_dist=None, Kmeans=False):\n",
    "\n",
    "  ## 1. buscar o cluster do user test\n",
    "  test_user = user_profiles_train.loc[user]\n",
    "  user_idx = user_profiles_train.index.get_loc(user)\n",
    "\n",
    "  ## 2. buscar os users do mesmo cluster do user test\n",
    "  user_cluster = model.labels_[user_idx]\n",
    "  users = [id for id, cluster in enumerate(model.labels_) if cluster == user_cluster and id != user_idx]\n",
    "\n",
    "  if len(users) < 5: # 2.5. vamos buscar mais users\n",
    "    if Kmeans:\n",
    "      user_dist = model_dist[user_profiles_train.index.get_loc(user)]\n",
    "      sim_cluster = sorted(range(len(user_dist)), key=lambda x: user_dist[x])[1:2][0] # vamos sÃ³ buscar 1 cluster extra\n",
    "      users.extend([id for id, cluster in enumerate(model.labels_) if cluster == sim_cluster])\n",
    "\n",
    "    else:\n",
    "      dist = euclidean_distances([test_user], user_profiles_train.values)[0] # aqui vamos ver todos os users\n",
    "      sorted_users = sorted(range(len(dist)), key=lambda x: dist[x])\n",
    "\n",
    "      # filtrar para nÃ£o haver repetiÃ§Ã£o de users, ou inserÃ§Ã£o do test_user\n",
    "      sorted_users_new = [x for x in sorted_users if x not in users]\n",
    "\n",
    "      for user in sorted_users_new:\n",
    "        if len(users) < 5 and user != user_idx:\n",
    "          users.append(user)\n",
    "\n",
    "  ## 3. calcular semelhanÃ§as entre o user test e os users dos clusters mais prÃ³ximos\n",
    "  users_similar = []\n",
    "  for user_id in users:\n",
    "      user_profile = user_profiles_train.iloc[user_id]\n",
    "      similarity_score = 1 / (1 + euclidean_distances([test_user], [user_profile])[0])\n",
    "      users_similar.append((user_id, similarity_score))\n",
    "\n",
    "  ## 4. ordem decrescente para o valor de semelhanÃ§a\n",
    "  users_similar = sorted(users_similar, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  return users_similar[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items_from_users(user, model, ratings_matrix, k=10):\n",
    "  rec_users = similar_users(user, model, n=5)\n",
    "  weights = [1 / (i+1) for i, _ in enumerate(rec_users)]\n",
    "\n",
    "  users = list(user[0] for user in rec_users)\n",
    "  rec_users_items = ratings_matrix.iloc[users] # items of similar users\n",
    "  item_count = rec_users_items.count(axis=0) # number of ratings of each item within the similar user\n",
    "\n",
    "  weighted_avg_rating = rec_users_items.multiply(weights, axis=0).mean(axis=0)\n",
    "  weighted_scores = weighted_avg_rating * item_count # calculate a weighted score using the weighted ratings and the number of ratings per item\n",
    "\n",
    "  recommended_items = list(weighted_scores.nlargest(k).items())\n",
    "  # print(recommended_items)\n",
    "\n",
    "  return recommended_items\n",
    "\n",
    "def recommend_items_from_cf(predictions, threshold=0):\n",
    "  preds = list()\n",
    "  for _, iid, _, est, _ in predictions:\n",
    "    preds.append((iid, est))\n",
    "\n",
    "  preds.sort(key=lambda x: x[1], reverse=True)\n",
    "  recommended_items = [(iid, est) for (iid, est) in preds if est >= threshold]\n",
    "  # print(recommended_items)\n",
    "\n",
    "  return recommended_items\n",
    "\n",
    "def evaluation(ratings_matrix, recommendation, relevant, k=10):\n",
    "  precisions, recalls, f1scores = [], [], []\n",
    "\n",
    "  for i in range(1, k+1):\n",
    "  # for i in range(k, k+1):\n",
    "    precision, recall, f1score = precision_recall_at_k(recommendation, relevant, i)\n",
    "    # print(\"K =\", i, \"- Precision:\", precision, \", Recall:\", recall, \", F1 Score:\", f1score)\n",
    "\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1scores.append(f1score)\n",
    "\n",
    "  return precisions, recalls, f1scores\n",
    "\n",
    "def precision_recall_at_k(recommendation, relevant, k=10):\n",
    "  relevant_items = list(item[1] for item in relevant if item[2] > 0)\n",
    "  # print(\"ITENS RELEVANTES:\", relevant_items)\n",
    "\n",
    "\n",
    "  # for i in range(k):\n",
    "  #   print(f\"K = {i+1} - \", recommendation[:i+1])\n",
    "  # print()\n",
    "\n",
    "  rel = len(relevant_items) # total number of relevant items to the user\n",
    "  rel_rec = np.sum(np.isin(recommendation[:k], relevant_items)) # number of relevant items recommended to the user\n",
    "\n",
    "  # k is the total number of recommended items to the user\n",
    "  precision = rel_rec / k # number of relevant items recommended to the user / total number of recommended items to the user\n",
    "  recall = rel_rec / rel if rel != 0 else 1  # number of relevant items recommended to the user / total number of relevant items to the user\n",
    "  f1score = (2*precision*recall) / (precision+recall) if (precision+recall) != 0 else 0.0\n",
    "\n",
    "  return precision, recall, f1score\n",
    "\n",
    "def avg_metrics(precisions, recalls, f1scores, k):\n",
    "  precisions_avg, recall_avg, f1score_avg = [], [], []\n",
    "  for i in range(k):\n",
    "    precision, recall, f1score = [], [], []\n",
    "\n",
    "    for prec, rec, f1 in zip(precisions, recalls, f1scores):\n",
    "      precision.append(prec[i])\n",
    "      recall.append(rec[i])\n",
    "      f1score.append(f1[i])\n",
    "\n",
    "    precisions_avg.append(np.round(np.mean(precision), 4))\n",
    "    recall_avg.append(np.round(np.mean(recall), 4))\n",
    "    f1score_avg.append(np.round(np.mean(f1score), 4))\n",
    "\n",
    "  return precisions_avg, recall_avg, f1score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_system(dataset, ratings_matrix, k=10):\n",
    "\n",
    "  precisions_final, recalls_final, f1scores_final = [], [], []\n",
    "  for i in range(5):\n",
    "    trainset, testset = split_emorecsys(dataset)\n",
    "\n",
    "    ## CF\n",
    "    model_cf = NMF(**cf_params)\n",
    "    # model_cf = CoClustering(**cf_params2)\n",
    "    model_cf.fit(trainset)\n",
    "\n",
    "    ## Demographic-based\n",
    "    model_demo = SpectralClustering(**demo_params)\n",
    "    model_demo.fit(user_profiles_train)\n",
    "\n",
    "    test_users = list(set(item[0] for item in testset))\n",
    "    for user in test_users:\n",
    "\n",
    "      user_relevant = list(item for item in testset if item[0] == user) # relevant items for evaluation\n",
    "      recommended_items_by_users = recommend_items_from_users(user=user, model=model_demo, ratings_matrix=ratings_matrix, k=20)\n",
    "\n",
    "      # print(\"USER\", user)\n",
    "      # print(recommended_items_by_users)\n",
    "\n",
    "      predictions_cf = [model_cf.predict(user, iid) for iid, _ in recommended_items_by_users] # predicting for the items suggested by demographic-based system\n",
    "      recommended_items_by_cf =  recommend_items_from_cf(predictions_cf, threshold=0.5)\n",
    "      # print(recommended_items_by_cf)\n",
    "      # print()\n",
    "\n",
    "      weighted_scores = []\n",
    "      for demo_score, cf_score in zip(recommended_items_by_users, recommended_items_by_cf):\n",
    "        # print(demo_score)\n",
    "        # print(cf_score)\n",
    "        weighted_score = (min(demo_score[1], 1)*0.5) + (min(cf_score[1], 1)*0.5)\n",
    "        weighted_scores.append((demo_score[0], weighted_score))\n",
    "\n",
    "      weighted_scores.sort(key=lambda x: x[1], reverse=True) # descending sort\n",
    "      # print(weighted_scores)\n",
    "\n",
    "      precisions, recalls, f1scores = evaluation(ratings_matrix, weighted_scores, user_relevant, k=k)\n",
    "\n",
    "      precisions_final.append(precisions)\n",
    "      recalls_final.append(recalls)\n",
    "      f1scores_final.append(f1scores)\n",
    "\n",
    "  precision_avg, recall_avg, f1score_avg = avg_metrics(precisions_final, recalls_final, f1scores_final, k)\n",
    "\n",
    "  for i in range(k):\n",
    "    print(f\"K = {i+1} - Precision: {precision_avg[i]}, Recall: {recall_avg[i]}, F1 Score: {f1score_avg[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lmir\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lmir\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lmir\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lmir\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lmir\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1 - Precision: 0.0209, Recall: 0.0112, F1 Score: 0.0131\n",
      "K = 2 - Precision: 0.0196, Recall: 0.0186, F1 Score: 0.018\n",
      "K = 3 - Precision: 0.0209, Recall: 0.0288, F1 Score: 0.0232\n",
      "K = 4 - Precision: 0.0215, Recall: 0.0407, F1 Score: 0.0271\n",
      "K = 5 - Precision: 0.0216, Recall: 0.0521, F1 Score: 0.0295\n",
      "K = 6 - Precision: 0.0223, Recall: 0.0673, F1 Score: 0.0324\n",
      "K = 7 - Precision: 0.021, Recall: 0.073, F1 Score: 0.0317\n",
      "K = 8 - Precision: 0.0206, Recall: 0.0798, F1 Score: 0.0318\n",
      "K = 9 - Precision: 0.0206, Recall: 0.0881, F1 Score: 0.0326\n",
      "K = 10 - Precision: 0.02, Recall: 0.0947, F1 Score: 0.0323\n"
     ]
    }
   ],
   "source": [
    "hybrid_system(ratings_df, ratings_matrix_like) # nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_n_items(trainset, testset, similarity_df, n=86, threshold=0.5):\n",
    "\n",
    "    test_users = set(item[0] for item in testset)\n",
    "    recommendations = {}\n",
    "\n",
    "    train_df = pd.DataFrame(trainset.all_ratings(), columns=['uid', 'iid', 'rating'])\n",
    "    train_df['uid'] = train_df['uid'].apply(trainset.to_raw_uid)\n",
    "    train_df['iid'] = train_df['iid'].apply(trainset.to_raw_iid)\n",
    "\n",
    "    test_df = pd.DataFrame(testset, columns=['uid', 'iid', 'rating'])\n",
    "\n",
    "    for user in test_users:\n",
    "        user_train_df = train_df[train_df['uid'] == user]\n",
    "        items_seen_by_user = set(user_train_df['iid'])\n",
    "        liked_items_train = set(user_train_df[user_train_df['rating'] > 0]['iid'])\n",
    "\n",
    "        user_test_df = test_df[(test_df['uid'] == user) & (test_df['rating'] > 0)]\n",
    "        relevant_items = set(user_test_df['iid'])\n",
    "\n",
    "        all_sim_items = set()\n",
    "        for item in liked_items_train:\n",
    "            top_sim_items = similar_items(item, similarity_df, items_seen_by_user, n)\n",
    "            all_sim_items.update(top_sim_items)\n",
    "\n",
    "        average_similarity_scores = get_average_similarity_scores(relevant_items, all_sim_items, similarity_df, threshold)\n",
    "        \n",
    "        top_n_items = get_top_n_items(average_similarity_scores, n)\n",
    "        \n",
    "        # Convert top_n_items to a list of tuples with (item_id, similarity_score)\n",
    "        recommendations[user] = [(item, average_similarity_scores[item]) for item in top_n_items]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "def get_average_similarity_scores(liked_items_test, all_sim_items, similarity_df, threshold):\n",
    "    similarity_dict = similarity_df.set_index(['Target ID', 'Comparison ID'])['Similarity Score'].to_dict()\n",
    "    similarity_sums = defaultdict(float)\n",
    "    similarity_counts = defaultdict(int)\n",
    "\n",
    "    for liked_item in liked_items_test:\n",
    "        for sim_item in all_sim_items:\n",
    "            score = similarity_dict.get((liked_item, sim_item)) or similarity_dict.get((sim_item, liked_item))\n",
    "            if score is not None and score >= threshold:\n",
    "                similarity_sums[sim_item] += score\n",
    "                similarity_counts[sim_item] += 1\n",
    "\n",
    "    average_similarity_scores = {item: similarity_sums[item] / similarity_counts[item] for item in similarity_sums}\n",
    "    return dict(sorted(average_similarity_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def get_top_n_items(average_similarity_scores, n):\n",
    "    # Sort by score in descending order and return the top N items\n",
    "    return list(average_similarity_scores.keys())[:n]\n",
    "\n",
    "def similar_items(item_id, similarity_df, items_seen_by_user, top_n):\n",
    "    # Filtrando diretamente com pandas e ordenando\n",
    "    similar_items_df = similarity_df[similarity_df['Target ID'] == item_id].nlargest(top_n, 'Similarity Score')\n",
    "    \n",
    "    seen_items = set(items_seen_by_user)\n",
    "    similar_item_ids = set()\n",
    "\n",
    "    for comparison_id in similar_items_df['Comparison ID']:\n",
    "        if comparison_id != item_id and comparison_id not in seen_items and comparison_id not in similar_item_ids:\n",
    "            similar_item_ids.add(comparison_id)\n",
    "        if len(similar_item_ids) >= top_n:\n",
    "            break\n",
    "    \n",
    "    return list(similar_item_ids)[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_system_cb_cf(dataset, ratings_matrix, similarity_df, k=10):\n",
    "\n",
    "    precisions_final, recalls_final, f1scores_final = [], [], []\n",
    "\n",
    "    for i in range(5):\n",
    "        trainset, testset = split_emorecsys(dataset)\n",
    "\n",
    "        # Content-Based Recommendations\n",
    "        recommendations_cb = recommend_top_n_items(trainset, testset, similarity_df)\n",
    "\n",
    "        # Collaborative Filtering Recommendations\n",
    "        model_cf = NMF(**cf_params)\n",
    "        model_cf.fit(trainset)\n",
    "\n",
    "        test_users = list(set(item[0] for item in testset))\n",
    "\n",
    "        for user in test_users:\n",
    "            user_relevant = list(item for item in testset if item[0] == user) # Relevant items for evaluation\n",
    "\n",
    "            # Get content-based recommendations\n",
    "            recommended_items_by_cb = recommendations_cb.get(user, [])\n",
    "            # print('recommended_items_by_cb')\n",
    "            # print(recommended_items_by_cb)\n",
    "            # print(len(recommended_items_by_cb))\n",
    "\n",
    "            predictions_cf = [model_cf.predict(user, iid) for iid, _ in recommended_items_by_cb] # predicting for the items suggested by content-based system\n",
    "            recommended_items_by_cf =  recommend_items_from_cf(predictions_cf, threshold=0.5)\n",
    "            # print('recommended_items_by_cf')\n",
    "            # print(recommended_items_by_cf)\n",
    "            # print(len(recommended_items_by_cf))\n",
    "            # print() #DÃ¡ sempre 1 mega bom!\n",
    "\n",
    "            # Combine recommendations\n",
    "            weighted_scores = []\n",
    "            for cb_score, cf_score in zip(recommended_items_by_cb, recommended_items_by_cf):\n",
    "                # print(cb_score, cf_score)\n",
    "                if cb_score[0] != cf_score[0]: #os items recomendados nem sempre eram iguais entÃ£o achei que assim fazia sentido\n",
    "                    if cb_score[1] > cf_score[1]:\n",
    "                        weighted_scores.append((cb_score[0], cb_score[1]))\n",
    "                    else:\n",
    "                        weighted_scores.append((cf_score[0], cf_score[1]))\n",
    "                else:\n",
    "                    weighted_score = (min(cb_score[1], 1) * 0.5) + (min(cf_score[1], 1) * 0.5)\n",
    "                    weighted_scores.append((cb_score[0], weighted_score))\n",
    "\n",
    "            weighted_scores.sort(key=lambda x: x[1], reverse=True) # Descending sort\n",
    "\n",
    "            # Evaluate hybrid recommendations\n",
    "            precisions, recalls, f1scores = evaluation(ratings_matrix, weighted_scores, user_relevant, k=k)\n",
    "            \n",
    "            precisions_final.append(precisions)\n",
    "            recalls_final.append(recalls)\n",
    "            f1scores_final.append(f1scores)\n",
    "\n",
    "    precision_avg, recall_avg, f1score_avg = avg_metrics(precisions_final, recalls_final, f1scores_final, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"K = {i+1} - Precision: {precision_avg[i]:.4f}, Recall: {recall_avg[i]:.4f}, F1 Score: {f1score_avg[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1 - Precision: 0.3239, Recall: 0.1577, F1 Score: 0.2037\n",
      "K = 2 - Precision: 0.1847, Recall: 0.1763, F1 Score: 0.1735\n",
      "K = 3 - Precision: 0.1239, Recall: 0.1771, F1 Score: 0.1407\n",
      "K = 4 - Precision: 0.0929, Recall: 0.1771, F1 Score: 0.1180\n",
      "K = 5 - Precision: 0.0744, Recall: 0.1771, F1 Score: 0.1017\n",
      "K = 6 - Precision: 0.0620, Recall: 0.1771, F1 Score: 0.0893\n",
      "K = 7 - Precision: 0.0531, Recall: 0.1771, F1 Score: 0.0797\n",
      "K = 8 - Precision: 0.0465, Recall: 0.1771, F1 Score: 0.0720\n",
      "K = 9 - Precision: 0.0413, Recall: 0.1771, F1 Score: 0.0656\n",
      "K = 10 - Precision: 0.0372, Recall: 0.1771, F1 Score: 0.0602\n"
     ]
    }
   ],
   "source": [
    "hybrid_system_cb_cf(ratings_df, ratings_matrix_like, pixel_sim_resnet152_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_system_cb_cf(dataset, ratings_matrix, similarity_df, k=10):\n",
    "\n",
    "    precisions_final, recalls_final, f1scores_final = [], [], []\n",
    "\n",
    "    for i in range(5):\n",
    "        trainset, testset = split_emorecsys(dataset)\n",
    "\n",
    "        # Content-Based Recommendations\n",
    "        recommendations_cb = recommend_top_n_items(trainset, testset, similarity_df)\n",
    "\n",
    "        # Collaborative Filtering Recommendations\n",
    "        model_cf = NMF(**cf_params)\n",
    "        model_cf.fit(trainset)\n",
    "\n",
    "        test_users = list(set(item[0] for item in testset))\n",
    "\n",
    "        for user in test_users:\n",
    "            user_relevant = list(item for item in testset if item[0] == user) # Relevant items for evaluation\n",
    "\n",
    "            # Obtenha as recomendaÃ§Ãµes baseadas em conteÃºdo\n",
    "            recommended_items_by_cb = recommendations_cb.get(user, [])\n",
    "\n",
    "            # Preveja para os itens sugeridos pelo sistema baseado em conteÃºdo\n",
    "            predictions_cf = [model_cf.predict(user, iid) for iid, _ in recommended_items_by_cb]\n",
    "            recommended_items_by_cf_dict = {iid: score for (iid, score) in recommend_items_from_cf(predictions_cf, threshold=0.5)}\n",
    "\n",
    "            # Inicialize a lista de weighted_scores\n",
    "            weighted_scores = []\n",
    "\n",
    "            # Itere sobre as recomendaÃ§Ãµes de conteÃºdo e combine com as de CF se existir\n",
    "            for cb_score in recommended_items_by_cb:\n",
    "                iid_cb, score_cb = cb_score\n",
    "                # print(iid_cb)\n",
    "                if iid_cb in recommended_items_by_cf_dict:\n",
    "                    score_cf = recommended_items_by_cf_dict[iid_cb]\n",
    "                    weighted_score = (min(score_cb, 1) * 0.5) + (min(score_cf, 1) * 0.5)\n",
    "                    weighted_scores.append((iid_cb, weighted_score))\n",
    "                else:\n",
    "                    weighted_scores.append((iid_cb, score_cb))  # Caso nÃ£o exista cf pÃµe o item com o score do cb\n",
    "\n",
    "            weighted_scores.sort(key=lambda x: x[1], reverse=True) # sort descending\n",
    "\n",
    "            # Evaluate hybrid recommendations\n",
    "            precisions, recalls, f1scores = evaluation(ratings_matrix, weighted_scores, user_relevant, k=k)\n",
    "            \n",
    "            precisions_final.append(precisions)\n",
    "            recalls_final.append(recalls)\n",
    "            f1scores_final.append(f1scores)\n",
    "\n",
    "    precision_avg, recall_avg, f1score_avg = avg_metrics(precisions_final, recalls_final, f1scores_final, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"K = {i+1} - Precision: {precision_avg[i]:.4f}, Recall: {recall_avg[i]:.4f}, F1 Score: {f1score_avg[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1 - Precision: 0.7558, Recall: 0.3507, F1 Score: 0.4636\n",
      "K = 2 - Precision: 0.5442, Recall: 0.4796, F1 Score: 0.4952\n",
      "K = 3 - Precision: 0.3845, Recall: 0.5012, F1 Score: 0.4236\n",
      "K = 4 - Precision: 0.2887, Recall: 0.5016, F1 Score: 0.3575\n",
      "K = 5 - Precision: 0.2317, Recall: 0.5029, F1 Score: 0.3101\n",
      "K = 6 - Precision: 0.1930, Recall: 0.5029, F1 Score: 0.2733\n",
      "K = 7 - Precision: 0.1655, Recall: 0.5029, F1 Score: 0.2443\n",
      "K = 8 - Precision: 0.1448, Recall: 0.5029, F1 Score: 0.2209\n",
      "K = 9 - Precision: 0.1288, Recall: 0.5035, F1 Score: 0.2018\n",
      "K = 10 - Precision: 0.1160, Recall: 0.5035, F1 Score: 0.1856\n"
     ]
    }
   ],
   "source": [
    "hybrid_system_cb_cf(ratings_df, ratings_matrix_like, pixel_sim_resnet152_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_system_cb_cf(dataset, ratings_matrix, similarity_df, k=10):\n",
    "\n",
    "    precisions_final, recalls_final, f1scores_final = [], [], []\n",
    "\n",
    "    for i in range(5):\n",
    "        trainset, testset = split_emorecsys(dataset)\n",
    "\n",
    "        # Content-Based Recommendations\n",
    "        recommendations_cb = recommend_top_n_items(trainset, testset, similarity_df)\n",
    "\n",
    "        # Collaborative Filtering Recommendations\n",
    "        model_cf = NMF(**cf_params)\n",
    "        model_cf.fit(trainset)\n",
    "\n",
    "        test_users = list(set(item[0] for item in testset))\n",
    "\n",
    "        for user in test_users:\n",
    "            user_relevant = list(item for item in testset if item[0] == user) # Relevant items for evaluation\n",
    "\n",
    "            # Obtenha as recomendaÃ§Ãµes baseadas em conteÃºdo\n",
    "            recommended_items_by_cb = recommendations_cb.get(user, [])\n",
    "            recommended_items_by_cb_dict = {item: score for item, score in recommended_items_by_cb}\n",
    "\n",
    "            # Preveja para os itens sugeridos pelo sistema baseado em conteÃºdo\n",
    "            predictions_cf = [model_cf.predict(user, iid) for iid, _ in recommended_items_by_cb]\n",
    "            recommended_items_by_cf =  recommend_items_from_cf(predictions_cf, threshold=0.5)\n",
    "\n",
    "            # Inicialize a lista de weighted_scores\n",
    "            weighted_scores = []\n",
    "\n",
    "            # Itere sobre as recomendaÃ§Ãµes de conteÃºdo e combine com as de CF se existir\n",
    "            for cf_score in recommended_items_by_cf:\n",
    "                iid_cf, score_cf = cf_score\n",
    "                if iid_cf in recommended_items_by_cb_dict:\n",
    "                    score_cb = recommended_items_by_cb_dict[iid_cf]\n",
    "                    weighted_score = (min(score_cf, 1) * 0.5) + (min(score_cb, 1) * 0.5)\n",
    "                    weighted_scores.append((iid_cf, weighted_score))\n",
    "                else:\n",
    "                    weighted_scores.append((iid_cf, score_cf))  # Caso nÃ£o exista cf pÃµe o item com o score do cb\n",
    "\n",
    "            weighted_scores.sort(key=lambda x: x[1], reverse=True) # sort descending\n",
    "\n",
    "            # Evaluate hybrid recommendations\n",
    "            precisions, recalls, f1scores = evaluation(ratings_matrix, weighted_scores, user_relevant, k=k)\n",
    "            \n",
    "            precisions_final.append(precisions)\n",
    "            recalls_final.append(recalls)\n",
    "            f1scores_final.append(f1scores)\n",
    "\n",
    "    precision_avg, recall_avg, f1score_avg = avg_metrics(precisions_final, recalls_final, f1scores_final, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"K = {i+1} - Precision: {precision_avg[i]:.4f}, Recall: {recall_avg[i]:.4f}, F1 Score: {f1score_avg[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1 - Precision: 0.7374, Recall: 0.3448, F1 Score: 0.4542\n",
      "K = 2 - Precision: 0.5092, Recall: 0.4499, F1 Score: 0.4628\n",
      "K = 3 - Precision: 0.3587, Recall: 0.4691, F1 Score: 0.3948\n",
      "K = 4 - Precision: 0.2696, Recall: 0.4699, F1 Score: 0.3335\n",
      "K = 5 - Precision: 0.2157, Recall: 0.4699, F1 Score: 0.2885\n",
      "K = 6 - Precision: 0.1798, Recall: 0.4699, F1 Score: 0.2542\n",
      "K = 7 - Precision: 0.1541, Recall: 0.4699, F1 Score: 0.2273\n",
      "K = 8 - Precision: 0.1348, Recall: 0.4699, F1 Score: 0.2055\n",
      "K = 9 - Precision: 0.1200, Recall: 0.4706, F1 Score: 0.1878\n",
      "K = 10 - Precision: 0.1080, Recall: 0.4706, F1 Score: 0.1727\n"
     ]
    }
   ],
   "source": [
    "hybrid_system_cb_cf(ratings_df, ratings_matrix_like, pixel_sim_resnet152_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hibrido sem dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_system_cb_cf(dataset, ratings_matrix, similarity_df, k=10):\n",
    "\n",
    "    precisions_final, recalls_final, f1scores_final = [], [], []\n",
    "\n",
    "    for i in range(5):\n",
    "        trainset, testset = split_emorecsys(dataset)\n",
    "        # print(testset)\n",
    "\n",
    "        # Content-Based Recommendations\n",
    "        recommendations_cb = recommend_top_n_items(trainset, testset, similarity_df)\n",
    "\n",
    "        # Collaborative Filtering Recommendations\n",
    "        model_cf = NMF(**cf_params)\n",
    "        model_cf.fit(trainset)\n",
    "\n",
    "        test_users = list(set(item[0] for item in testset))\n",
    "\n",
    "        for user in test_users:\n",
    "            user_relevant = list(item for item in testset if item[0] == user) # Relevant items for evaluation\n",
    "\n",
    "            # print(\"USER\", user)\n",
    "\n",
    "            # Get content-based recommendations\n",
    "            recommended_items_by_cb = recommendations_cb.get(user, [])\n",
    "            # print('recommend_items_by_cb')\n",
    "            # print(recommended_items_by_cb)\n",
    "            \n",
    "            # Predict ratings for items suggested \n",
    "            predictions_cf = [model_cf.predict(user, item[1]) for item in user_relevant]\n",
    "            recommended_items_by_cf_dict = {iid: score for (iid, score) in recommend_items_from_cf(predictions_cf, threshold=0.5)}\n",
    "            \n",
    "            # print('recommend_items_by_cf')\n",
    "            # print(recommended_items_by_cf_dict)\n",
    "            # print()\n",
    "\n",
    "            # Combine recommendations\n",
    "            weighted_scores = []\n",
    "            for cb_score in recommended_items_by_cb:\n",
    "                iid_cb, score_cb = cb_score\n",
    "                # print(iid_cb)\n",
    "                if iid_cb in recommended_items_by_cf_dict:\n",
    "                    score_cf = recommended_items_by_cf_dict[iid_cb]\n",
    "                    weighted_score = (min(score_cb, 1) * 0.5) + (min(score_cf, 1) * 0.5)\n",
    "                    weighted_scores.append((iid_cb, weighted_score))\n",
    "                else:\n",
    "                    weighted_scores.append((iid_cb, score_cb))  # Caso nÃ£o exista cf pÃµe o item com o score do cb\n",
    "            weighted_scores.sort(key=lambda x: x[1], reverse=True) # Descending sort\n",
    "\n",
    "            # Evaluate hybrid recommendations\n",
    "            precisions, recalls, f1scores = evaluation(ratings_matrix, weighted_scores, user_relevant, k=k)\n",
    "            \n",
    "            precisions_final.append(precisions)\n",
    "            recalls_final.append(recalls)\n",
    "            f1scores_final.append(f1scores)\n",
    "\n",
    "    precision_avg, recall_avg, f1score_avg = avg_metrics(precisions_final, recalls_final, f1scores_final, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"K = {i+1} - Precision: {precision_avg[i]:.4f}, Recall: {recall_avg[i]:.4f}, F1 Score: {f1score_avg[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1 - Precision: 0.7755, Recall: 0.3601, F1 Score: 0.4753\n",
      "K = 2 - Precision: 0.5564, Recall: 0.4916, F1 Score: 0.5065\n",
      "K = 3 - Precision: 0.3947, Recall: 0.5153, F1 Score: 0.4347\n",
      "K = 4 - Precision: 0.2963, Recall: 0.5157, F1 Score: 0.3669\n",
      "K = 5 - Precision: 0.2371, Recall: 0.5157, F1 Score: 0.3173\n",
      "K = 6 - Precision: 0.1975, Recall: 0.5157, F1 Score: 0.2796\n",
      "K = 7 - Precision: 0.1693, Recall: 0.5157, F1 Score: 0.2500\n",
      "K = 8 - Precision: 0.1482, Recall: 0.5157, F1 Score: 0.2260\n",
      "K = 9 - Precision: 0.1317, Recall: 0.5157, F1 Score: 0.2063\n",
      "K = 10 - Precision: 0.1185, Recall: 0.5157, F1 Score: 0.1897\n"
     ]
    }
   ],
   "source": [
    "hybrid_system_cb_cf(ratings_df, ratings_matrix_like, pixel_sim_resnet152_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_system_cb_cf(dataset, ratings_matrix, similarity_df, k=10):\n",
    "\n",
    "    precisions_final, recalls_final, f1scores_final = [], [], []\n",
    "\n",
    "    for i in range(5):\n",
    "        trainset, testset = split_emorecsys(dataset)\n",
    "        # print(testset)\n",
    "\n",
    "        # Content-Based Recommendations\n",
    "        recommendations_cb = recommend_top_n_items(trainset, testset, similarity_df, n= 86, threshold = 0 )\n",
    "\n",
    "        # Collaborative Filtering Recommendations\n",
    "        model_cf = NMF(**cf_params)\n",
    "        model_cf.fit(trainset)\n",
    "\n",
    "        test_users = list(set(item[0] for item in testset))\n",
    "\n",
    "        for user in test_users:\n",
    "            user_relevant = list(item for item in testset if item[0] == user) # Relevant items for evaluation\n",
    "\n",
    "            # print(\"USER\", user)\n",
    "\n",
    "            # Get content-based recommendations\n",
    "            recommended_items_by_cb = recommendations_cb.get(user, [])\n",
    "            recommended_items_by_cb_dict = {item: score for item, score in recommended_items_by_cb}\n",
    "            # print('recommend_items_by_cb')\n",
    "            # print(recommended_items_by_cb_dict)\n",
    "            \n",
    "            # Predict ratings for items suggested \n",
    "            predictions_cf = [model_cf.predict(user, item[1]) for item in user_relevant]\n",
    "            recommended_items_by_cf = recommend_items_from_cf(predictions_cf, threshold=0.5) \n",
    "            \n",
    "            # print('recommend_items_by_cf')\n",
    "            # print(recommended_items_by_cf)\n",
    "            # print()\n",
    "\n",
    "            # Combine recommendations\n",
    "            weighted_scores = []\n",
    "\n",
    "            for cf_score in recommended_items_by_cf:\n",
    "                iid_cf, score_cf = cf_score\n",
    "                if iid_cf in recommended_items_by_cb_dict:\n",
    "                    score_cb = recommended_items_by_cb_dict[iid_cf]\n",
    "                    weighted_score = (min(score_cf, 1) * 0.5) + (min(score_cb, 1) * 0.5)\n",
    "                    weighted_scores.append((iid_cf, weighted_score))\n",
    "                else:\n",
    "                    weighted_scores.append((iid_cf, score_cf))  # Caso nÃ£o exista cf pÃµe o item com o score do cb\n",
    "            weighted_scores.sort(key=lambda x: x[1], reverse=True) # Descending sort\n",
    "\n",
    "            # Evaluate hybrid recommendations\n",
    "            precisions, recalls, f1scores = evaluation(ratings_matrix, weighted_scores, user_relevant, k=k)\n",
    "            \n",
    "            precisions_final.append(precisions)\n",
    "            recalls_final.append(recalls)\n",
    "            f1scores_final.append(f1scores)\n",
    "\n",
    "    precision_avg, recall_avg, f1score_avg = avg_metrics(precisions_final, recalls_final, f1scores_final, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"K = {i+1} - Precision: {precision_avg[i]:.4f}, Recall: {recall_avg[i]:.4f}, F1 Score: {f1score_avg[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1 - Precision: 0.8638, Recall: 0.4094, F1 Score: 0.5339\n",
      "K = 2 - Precision: 0.8049, Recall: 0.7080, F1 Score: 0.7299\n",
      "K = 3 - Precision: 0.7284, Recall: 0.9370, F1 Score: 0.7981\n",
      "K = 4 - Precision: 0.5463, Recall: 0.9370, F1 Score: 0.6735\n",
      "K = 5 - Precision: 0.4371, Recall: 0.9370, F1 Score: 0.5828\n",
      "K = 6 - Precision: 0.3642, Recall: 0.9370, F1 Score: 0.5138\n",
      "K = 7 - Precision: 0.3122, Recall: 0.9370, F1 Score: 0.4595\n",
      "K = 8 - Precision: 0.2732, Recall: 0.9370, F1 Score: 0.4156\n",
      "K = 9 - Precision: 0.2428, Recall: 0.9370, F1 Score: 0.3794\n",
      "K = 10 - Precision: 0.2185, Recall: 0.9370, F1 Score: 0.3490\n"
     ]
    }
   ],
   "source": [
    "hybrid_system_cb_cf(ratings_df, ratings_matrix_like, pixel_sim_resnet152_df, k=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_system_switching(dataset, ratings_matrix, similarity_df, k=10, positive_interaction_threshold = 8):\n",
    "    \n",
    "    precisions_final, recalls_final, f1scores_final = [], [], []\n",
    "\n",
    "    for i in range(5):\n",
    "        trainset, testset = split_emorecsys(dataset)\n",
    "\n",
    "        # Content-Based Recommendations\n",
    "        recommendations_cb = recommend_top_n_items(trainset, testset, similarity_df)\n",
    "\n",
    "        # Collaborative Filtering Recommendations\n",
    "        model_cf = NMF(**cf_params)\n",
    "        model_cf.fit(trainset)\n",
    "\n",
    "        test_users = list(set(item[0] for item in testset))\n",
    "        recommendations_final = {}\n",
    "\n",
    "        # Convert trainset to a suitable format for interaction counting\n",
    "        interactions = defaultdict(int)\n",
    "        for user, _, rating in trainset.all_ratings():\n",
    "            # print(rating)\n",
    "            if rating > 0.5:\n",
    "                interactions[user] += 1\n",
    "        \n",
    "        for user in test_users:\n",
    "            user_relevant = [item for item in testset if item[0] == user]  # Relevant items for evaluation\n",
    "            user_interactions = interactions.get(user, 0)\n",
    "            # print(user_interactions)\n",
    "\n",
    "            if user_interactions < positive_interaction_threshold:\n",
    "                # Use Content-Based for users with few interactions\n",
    "                recommendations = recommendations_cb.get(user, [])\n",
    "            else:\n",
    "                # Use Collaborative Filtering for users with sufficient interactions\n",
    "                predictions_cf = [model_cf.predict(user, item[1]) for item in user_relevant]\n",
    "                recommendations = recommend_items_from_cf(predictions_cf, threshold=0.5) \n",
    "\n",
    "            # Update recommendations_final with the final recommendations for the user\n",
    "            recommendations_final[user] = recommendations\n",
    "            # print(recommendations_final[user])\n",
    "\n",
    "            # Evaluate switching recommendations\n",
    "            precisions, recalls, f1scores = evaluation(ratings_matrix, recommendations_final[user], user_relevant, k=k)\n",
    "\n",
    "            precisions_final.append(precisions)\n",
    "            recalls_final.append(recalls)\n",
    "            f1scores_final.append(f1scores)\n",
    "\n",
    "    precision_avg, recall_avg, f1score_avg = avg_metrics(precisions_final, recalls_final, f1scores_final, k)\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"K = {i+1} - Precision: {precision_avg[i]:.4f}, Recall: {recall_avg[i]:.4f}, F1 Score: {f1score_avg[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_system_switching(ratings_df, ratings_matrix_like,pixel_sim_resnet152_df ,k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
